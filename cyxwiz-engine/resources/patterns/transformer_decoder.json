{
  "id": "transformer_decoder",
  "name": "Transformer Decoder",
  "description": "Transformer decoder stack with masked self-attention and cross-attention. Suitable for autoregressive generation tasks like language modeling.",
  "category": "Transformer",
  "tags": ["transformer", "attention", "decoder", "gpt", "generation", "language-model"],
  "parameters": [
    {
      "name": "d_model",
      "type": "int",
      "default_value": "256",
      "description": "Model dimension (embedding size)",
      "min_value": "64",
      "max_value": "2048"
    },
    {
      "name": "num_heads",
      "type": "int",
      "default_value": "8",
      "description": "Number of attention heads",
      "min_value": "1",
      "max_value": "32"
    },
    {
      "name": "ff_dim",
      "type": "int",
      "default_value": "512",
      "description": "Feed-forward hidden dimension",
      "min_value": "64",
      "max_value": "4096"
    },
    {
      "name": "vocab_size",
      "type": "int",
      "default_value": "10000",
      "description": "Vocabulary size",
      "min_value": "100",
      "max_value": "100000"
    },
    {
      "name": "dropout_rate",
      "type": "float",
      "default_value": "0.1",
      "description": "Dropout rate",
      "min_value": "0.0",
      "max_value": "0.5"
    }
  ],
  "template": {
    "nodes": [
      {"id": "input", "type": "DatasetInput", "name": "Token Input", "pos": [0, 200]},
      {"id": "embed", "type": "Embedding", "name": "Token Embedding", "pos": [200, 200], "params": {"vocab_size": "$vocab_size", "embedding_dim": "$d_model"}},
      {"id": "pos_enc", "type": "PositionalEncoding", "name": "Positional Encoding", "pos": [400, 200], "params": {"d_model": "$d_model"}},
      {"id": "dropout1", "type": "Dropout", "name": "Dropout", "pos": [600, 200], "params": {"rate": "$dropout_rate"}},
      {"id": "dec1", "type": "TransformerDecoder", "name": "Decoder Layer 1", "pos": [800, 200], "params": {"d_model": "$d_model", "num_heads": "$num_heads", "ff_dim": "$ff_dim", "dropout": "$dropout_rate"}},
      {"id": "dec2", "type": "TransformerDecoder", "name": "Decoder Layer 2", "pos": [1000, 200], "params": {"d_model": "$d_model", "num_heads": "$num_heads", "ff_dim": "$ff_dim", "dropout": "$dropout_rate"}},
      {"id": "dec3", "type": "TransformerDecoder", "name": "Decoder Layer 3", "pos": [1200, 200], "params": {"d_model": "$d_model", "num_heads": "$num_heads", "ff_dim": "$ff_dim", "dropout": "$dropout_rate"}},
      {"id": "ln", "type": "LayerNorm", "name": "Layer Norm", "pos": [1400, 200]},
      {"id": "output_layer", "type": "Dense", "name": "Output Projection", "pos": [1600, 200], "params": {"units": "$vocab_size"}},
      {"id": "softmax", "type": "Softmax", "name": "Softmax", "pos": [1800, 200]},
      {"id": "output", "type": "Output", "name": "Output", "pos": [2000, 200]}
    ],
    "links": [
      {"from": "input", "to": "embed"},
      {"from": "embed", "to": "pos_enc"},
      {"from": "pos_enc", "to": "dropout1"},
      {"from": "dropout1", "to": "dec1"},
      {"from": "dec1", "to": "dec2"},
      {"from": "dec2", "to": "dec3"},
      {"from": "dec3", "to": "ln"},
      {"from": "ln", "to": "output_layer"},
      {"from": "output_layer", "to": "softmax"},
      {"from": "softmax", "to": "output"}
    ]
  }
}
